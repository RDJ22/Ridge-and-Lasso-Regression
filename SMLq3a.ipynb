{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question1.1) Derive the closed-form solution for ùõΩ (the coefficient estimates) for ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In ridge regression, we aim to minimize the penalized least-squares objective:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left( \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "### This expression can be expanded as follows:\n",
    "\n",
    "###  Step 1: Set up the Objective Function\n",
    "\n",
    "1. **Least-Squares Term**: The term \n",
    "$$ \\|y - X\\beta\\|_2^2\\ $$ \n",
    "is the sum of squared errors, given by:\n",
    "   $$\n",
    "   \\|y - X\\beta\\|_2^2 = (y - X\\beta)^{'} (y - X\\beta)\n",
    "   $$\n",
    "\n",
    "2. **Penalty Term**: The penalty term \n",
    "$$ \\|\\beta\\|_2^2\\ $$ \n",
    "#### is the squared L^2-norm of Œ≤, given by:\n",
    "   $$\n",
    "   \\|\\beta\\|_2^2 = \\beta^{'} \\beta\n",
    "   $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Expand the Objective Function\n",
    "\n",
    "#### Expanding the objective function gives:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left( (y - X\\beta)^{'} (y - X\\beta) + \\lambda \\beta^{'} \\beta \\right)\n",
    "$$\n",
    "\n",
    "#### Expanding:\n",
    "$$ (y - X\\beta)^{'} (y - X\\beta)\\ $$ \n",
    "#### yields:\n",
    "\n",
    "$$\n",
    "(y - X\\beta)^{'} (y - X\\beta) = y^{'} y - 2\\beta^{'} X' y + \\beta^{'} X' X \\beta\n",
    "$$\n",
    "\n",
    "#### Thus, the objective function becomes:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left( y^{'} y - 2\\beta^{'} X' y + \\beta^{'} X' X \\beta + \\lambda \\beta^{'} \\beta \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Take the Derivative with Respect to ùõΩ and Set it to Zero\n",
    "\n",
    "#### To find the optimal ùõΩ, take the derivative of the objective function with respect to ùõΩ and set it equal to zero:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\beta} \\left( y^{'} y - 2\\beta^{'} X' y + \\beta^{'} X' X \\beta + \\lambda \\beta^{'} \\beta \\right) = 0\n",
    "$$\n",
    "\n",
    "#### Calculating the derivative term by term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. The derivative of y'y with respect to ùõΩ is 0 since it does not involve Œ≤.\n",
    "##### b. The derivative of -2Œ≤'X'y with respect to Œ≤ is -2X'y\n",
    "##### c. The derivative of Œ≤'X'XŒ≤ with respect to Œ≤ is 2X'XŒ≤\n",
    "##### d. The derivative of ŒªŒ≤'Œ≤ with respect to Œ≤ is 2ŒªŒ≤\n",
    "\n",
    "#### Putting these together, we get:\n",
    "\n",
    "$$\n",
    "-2 X' y + 2 X' X \\beta + 2 \\lambda \\beta = 0\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Solve for Œ≤\n",
    "\n",
    "#### Rearranging terms to isolate Œ≤:\n",
    "\n",
    "$$\n",
    "X' X \\beta + \\lambda \\beta = X' y\n",
    "$$\n",
    "\n",
    "#### Factor out Œ≤:\n",
    "\n",
    "$$\n",
    "(X' X + \\lambda I) \\beta = X' y\n",
    "$$\n",
    "\n",
    "#### where I is the p*p identity matrix.\n",
    "\n",
    "#### Now, multiply both sides by the inverse of (X'X +  ŒªI) to solve for Œ≤:\n",
    "\n",
    "$$\n",
    "\\beta = (X' X + \\lambda I)^{-1} X' y\n",
    "$$\n",
    "\n",
    "#### This is the closed-form solution for Œ≤ in ridge regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2) Show that with regularization ( ùúÜ > 0), the estimate for ùõΩ is always invertible, even if ùëã ‚Ä≤ùëã is not full rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When Œª > 0, the matrix X' X + ŒªI is guaranteed to be invertible. This is because adding ŒªI with Œª > 0 to X' X increases the eigenvalues of X' X by Œª, ensuring that none of the eigenvalues are zero.\n",
    "\n",
    "##### a. If X' X is not full rank: Some eigenvalues of X' X might be zero, making it non-invertible.\n",
    "##### b. Effect of ŒªI: Adding ŒªI shifts all eigenvalues of X' X by Œª, making them strictly positive for Œª > 0.\n",
    "\n",
    "#### Thus, X' X + Œª I is always invertible when Œª > 0, ensuring that Œ≤ has a unique solution in ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<brb>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
